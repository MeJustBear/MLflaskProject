# Модель классификации новостей по категориям

## API
### Перед началом

### Сборка
Клонируем репозиторий, собираем образ с помощью compose.

```
$ git clone https://github.com/MeJustBear/ml-flask-test-task
$ cd cd ml-flask-test-task/
$ docker-compose up
```

### Запуск

```
 $ docker run --name my-container -d -p 8080:8080 ml-flask-test-task
```

### Функции внутри


## Модель
Перед началом обучения, необходимо "очистить данные". В данном случае, все новые абзаци обозначаются латинской "n" после знака окончания предложения, а также между двумя "n", заключаются цитаты.
Для очищения обучающих данных использовали 2 регулярных выражения.

```(python)
regex_cite = re.compile(r'[n](?P<word>[^n]+)[n]')
regex_par = re.compile(r'(?P<sign>[.;])[n](?P<word>[\S]+)')
for i in range(len(texts)):
  texts[i] = re.sub(regex_cite, r' \g<word> ', re.sub(regex_par, r'\g<sign> \g<word> ', texts[i]))
```

Для представления слов в удобной для нейросети форме, используется класс из пакета препроцессинга keras.

```(python)
from tensorflow.keras.preprocessing.text import Tokenizer
tokenizer = Tokenizer(num_words=maxWordsCount, filters='«»!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n', lower=True, split=' ', oov_token='unknown', char_level=False)
```

Каждому слову присвается его уникальный номер.

```(python)
tokenizer.fit_on_texts(texts)
```

Используется полносвязная модель. 

```(python)
modelE = Sequential()
```

Процесс обучения модели.

```(python)
Epoch 1/10
1699/1699 [==============================] - 196s 113ms/step - loss: 0.4772 - accuracy: 0.8589 - val_loss: 0.1426 - val_accuracy: 0.9581
Epoch 2/10
1699/1699 [==============================] - 194s 114ms/step - loss: 0.0369 - accuracy: 0.9899 - val_loss: 0.1419 - val_accuracy: 0.9666
Epoch 3/10
1699/1699 [==============================] - 193s 113ms/step - loss: 0.0235 - accuracy: 0.9938 - val_loss: 0.2275 - val_accuracy: 0.9540
Epoch 4/10
1699/1699 [==============================] - 194s 114ms/step - loss: 0.0294 - accuracy: 0.9920 - val_loss: 0.2066 - val_accuracy: 0.9576
Epoch 5/10
1699/1699 [==============================] - 193s 113ms/step - loss: 0.0208 - accuracy: 0.9939 - val_loss: 0.2059 - val_accuracy: 0.9691
Epoch 6/10
1699/1699 [==============================] - 194s 114ms/step - loss: 0.0157 - accuracy: 0.9956 - val_loss: 0.2400 - val_accuracy: 0.9651
Epoch 7/10
1699/1699 [==============================] - 193s 113ms/step - loss: 0.0167 - accuracy: 0.9956 - val_loss: 0.1790 - val_accuracy: 0.9670
Epoch 8/10
1699/1699 [==============================] - 194s 114ms/step - loss: 0.0135 - accuracy: 0.9959 - val_loss: 0.2037 - val_accuracy: 0.9677
Epoch 9/10
1699/1699 [==============================] - 194s 114ms/step - loss: 0.0118 - accuracy: 0.9966 - val_loss: 0.1964 - val_accuracy: 0.9693
Epoch 10/10
1699/1699 [==============================] - 194s 114ms/step - loss: 0.0102 - accuracy: 0.9971 - val_loss: 0.2094 - val_accuracy: 0.9687
```

Параметры полученной нейросети.

```(python)
modelE.summary()

Model: "sequential_5"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_5 (Embedding)      (None, 500, 200)          20000000  
_________________________________________________________________
spatial_dropout1d_1 (Spatial (None, 500, 200)          0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 100000)            0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 100000)            400000    
_________________________________________________________________
dense_1 (Dense)              (None, 1000)              100001000 
_________________________________________________________________
dropout (Dropout)            (None, 1000)              0         
_________________________________________________________________
batch_normalization_2 (Batch (None, 1000)              4000      
_________________________________________________________________
dense_2 (Dense)              (None, 10)                10010     
=================================================================
Total params: 120,415,010
Trainable params: 120,213,010
Non-trainable params: 202,000
_________________________________________________________________
```

### Результаты тестов

Для каждой категории была сформирована валидационная выборка. Затем, модель определяла её категорию, после вычислялись частоты правильного предсказания для текстов из каждой категории. 

```
Статистика: 
Тему: Без политики верно предсказывали в  99.326 % случаев
Тему: Бывший СССР верно предсказывали в  99.208 % случаев
Тему: Мероприятия RT верно предсказывали в  100.0 % случаев
Тему: Мир верно предсказывали в  99.3 % случаев
Тему: Наука верно предсказывали в  97.0 % случаев
Тему: Новости партнёров верно предсказывали в  100.0 % случаев
Тему: Пресс-релизы верно предсказывали в  100.0 % случаев
Тему: Россия верно предсказывали в  98.997 % случаев
Тему: Спорт верно предсказывали в  99.943 % случаев
Тему: Экономика верно предсказывали в  97.491 % случаев
Средняя точность составила:  99.1265 %
```
